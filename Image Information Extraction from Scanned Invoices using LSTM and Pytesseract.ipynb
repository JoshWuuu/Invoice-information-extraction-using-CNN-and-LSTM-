{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Information Extraction from Scanned Invoices using LSTM and Pytesseract.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNwLPc4j7eyt5SovPeXX7K3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MFV4Mb3CYJFI"},"source":["<!-- Ref: https://github.com/shreeshiv/AIESI -->\n","# Readme \n","*   File setup: first move all json and jpg files into one single folder, then aggregate all the json files to one single csv file for the future evaluation.\n","\n","*   Preprocess: turn the image into binary image (pixel value: 0 or 255, one channel), and then using erode and dilate to reduce the noise in the image. In addition, the project also filter out the connected components with too small area(area < 10).\n","\n","*   Recognized words: first find the bounding box of the text in the image. Then, loop thru the bounding box to recognize the word in the box using pytesseract, and return all the text in the image.\n","\n","*   LSTM field matching: use LSTM to estimate if each character belongs to the corresponding fields (company, date, address, total).\n","\n","*   Evalution: calculate and plot the f1 score and precision\n","\n","*   Main: estimate the f1 score and precision for training data, and output the result of validation data as csv file\n","\n","*   Future Direction:\n","\n","  1.   To further imporve the performance, we can try to adjust the kernel size at preprocessing step and text recognizing step (contour).\n","\n","  2.   Should train the model. The project use the pretrained model to do the LSTM field matching since the project can't train it properly. The reason is the project uses pytesseract to recognize words, and if the words weren't recognized correctly, the true labels can't be matched to the word vector, thus causing it untrainable. In the future, we should change word recognizing step to CNN structure and concatanate the CNN to the LSTM to update both weights together. In this way, the problem can be avoided.\n","\n","**I attach the validation result as csv file, and the pretrained model in the folder as well**"]},{"cell_type":"markdown","metadata":{"id":"Zzxp5pJhyDC8"},"source":["# Setup\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QE74g5lox1dF","executionInfo":{"status":"ok","timestamp":1616558073313,"user_tz":-480,"elapsed":19804,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}},"outputId":"3e49437f-f3f2-4f1b-ce19-9ba5484f5277"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5lFGsXMyHMH","executionInfo":{"status":"ok","timestamp":1616558095364,"user_tz":-480,"elapsed":15047,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}},"outputId":"f10e260e-2ccb-43c1-c4cc-c480b1a38fea"},"source":["!sudo apt install tesseract-ocr\n","!pip install colorama\n","!pip install -U git+https://github.com/madmaze/pytesseract.git"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  tesseract-ocr-eng tesseract-ocr-osd\n","The following NEW packages will be installed:\n","  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n","0 upgraded, 3 newly installed, 0 to remove and 30 not upgraded.\n","Need to get 4,795 kB of archives.\n","After this operation, 15.8 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n","Fetched 4,795 kB in 2s (2,418 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package tesseract-ocr-eng.\n","(Reading database ... 160980 files and directories currently installed.)\n","Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n","Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n","Selecting previously unselected package tesseract-ocr-osd.\n","Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n","Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n","Selecting previously unselected package tesseract-ocr.\n","Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n","Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n","Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n","Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n","Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.4\n","Collecting git+https://github.com/madmaze/pytesseract.git\n","  Cloning https://github.com/madmaze/pytesseract.git to /tmp/pip-req-build-l38gzla1\n","  Running command git clone -q https://github.com/madmaze/pytesseract.git /tmp/pip-req-build-l38gzla1\n","Requirement already satisfied, skipping upgrade: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract==0.3.7) (7.0.0)\n","Building wheels for collected packages: pytesseract\n","  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=14106 sha256=b449190da1fe97073f2b09009c3c0e62c757cb1cdf718b2f8db304650711ef97\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-teyoiy1f/wheels/be/2a/a1/a40dbc7e579dffb2be8bbc3243c491de2d132899309f008b1f\n","Successfully built pytesseract\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5lx9zne-yJxK","executionInfo":{"status":"ok","timestamp":1616558100671,"user_tz":-480,"elapsed":18126,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["import shutil\n","import pytesseract\n","import glob\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import torch \n","from torch import nn\n","import matplotlib.pyplot as plt\n","from skimage.filters import threshold_local\n","import json\n","from string import ascii_uppercase, digits, punctuation\n","import regex\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SE4qAHPEyr1C"},"source":["# Preprocess"]},{"cell_type":"code","metadata":{"id":"Nd4MgosgyuzO"},"source":["# aggregate the files into one file\n","def move_file(start, destination):\n","  files = glob.glob(start)\n","  for file in files: \n","    content = glob.glob(f\"{file}/*\")\n","    for i in content:\n","      if i.endswith('json') or i.endswith('jpg'):\n","        shutil.move(i, destination) \n","\n","move_file(\"/content/drive/MyDrive/Job/Infrred/dataset/val/*\", \"/content/drive/MyDrive/Job/Infrred/dataset/val_all\")\n","move_file(\"/content/drive/MyDrive/Job/Infrred/dataset/train/*\", \"/content/drive/MyDrive/Job/Infrred/dataset/train_all\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qza01n50F-q"},"source":["# aggregate all json file into one csv\n","def agg_json(destination):\n","  files = glob.glob(f\"/content/drive/MyDrive/Job/Infrred/dataset/train_all/*\")\n","  df = pd.DataFrame(columns=['index', 'company', 'date', 'address', 'total'])\n","  for file in files:\n","    if file.endswith('json'):\n","      index = file[-8:-5]\n","      if index.find('/') != -1:\n","        index = index[index.find('/')+1:]\n","      temp = pd.read_json(file, typ='series')\n","      temp = pd.DataFrame([temp])\n","      temp['index'] = index\n","      df = pd.concat([df, temp])\n","\n","  df.to_csv(destination)\n","\n","agg_json('/content/drive/MyDrive/Job/Infrred/dataset/train_all/all_json.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgLBNVfs1z08","executionInfo":{"status":"ok","timestamp":1616558100672,"user_tz":-480,"elapsed":9497,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def img_preprocess(path, kernel_size):\n","  # read the img as grey scale image\n","  img = cv2.imread(path, 0)\n","\n","  # turn into binary img using adaptive threshold\n","  T = threshold_local(img, 15, offset = 6, method = \"gaussian\")\n","  thresh = (img > T).astype(\"uint8\") * 255\n","  thresh = ~thresh\n","\n","  # erode and dilate the image\n","  kernel = np.ones((kernel_size, kernel_size), np.uint8)\n","  opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n","\n","  # find the connected components in the image\n","  nlabels, labels, stats, centroids = cv2.connectedComponentsWithStats(opening, None, None, None, 8, cv2.CV_32S)\n","  # filter out the small connected components (area < 10)\n","  area = stats[1:,-1]\n","  filtered_area = np.zeros((labels.shape), np.uint8)\n","  for i in range(0, nlabels-1):\n","    if area[i] >= 10:\n","      filtered_area[labels == i+1] = 255\n","  \n","  return filtered_area\n","  "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lo8mM9_540ii"},"source":["# Recognized words"]},{"cell_type":"code","metadata":{"id":"Eg7pq8PL4zVT","executionInfo":{"status":"ok","timestamp":1616558100673,"user_tz":-480,"elapsed":7561,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":[" def img_words(img): \n","\n","  # find contours\n","  kernel = np.ones((5, 20), np.uint8)\n","  img_dilate = cv2.dilate(img, kernel, iterations=1)\n","  contours, hir = cv2.findContours(img_dilate, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n","\n","  # find the corresponding bounding box for each contour\n","  rects = [cv2.boundingRect(cnt) for cnt in contours]\n","  sorted_rects = sorted(rects, key = lambda y:y[0])\n","  sorted_rects = sorted(sorted_rects, key = lambda y:y[1])\n","\n","  # loop thru each bounding box and find the word\n","  res = ''\n","  for i, rect in enumerate(sorted_rects):\n","    x, y, w, h = rect\n","    # if rect is too small, skip the rect\n","    if w<20 or h<20:\n","      continue\n","\n","    temp = img[y:y+h, x:x+w]\n","    temp = cv2.cvtColor(temp, cv2.COLOR_BAYER_BG2BGR)\n","    text = pytesseract.image_to_data(temp, config=r'--psm 6')\n","    text = text.split()\n","    index = 22\n","    while True:\n","      if index > len(text):\n","        break\n","      # no text if config == -1\n","      if int(text[index]) == -1:\n","        index += 11\n","      else:\n","        res += text[index+1]\n","        res += \" \"\n","        # since existence of the word, so index += 12\n","        index += 12\n","    res += '\\n'\n","  \n","  return res"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eh5v0DD59nUR"},"source":["# LSTM field matching"]},{"cell_type":"code","metadata":{"id":"jBjWAOGU9kIM","executionInfo":{"status":"ok","timestamp":1616558101477,"user_tz":-480,"elapsed":791,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["# define model, a pretrained and predefined model from reference\n","class MyModel(nn.Module):\n","  def __init__(self, vocab_size, embed_size, hidden_size):\n","    super().__init__()\n","    self.embed = nn.Embedding(vocab_size, embed_size)\n","    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=True)\n","    self.linear = nn.Linear(hidden_size * 2, 5)\n","\n","  def forward(self, inpt):\n","    embedded = self.embed(inpt)\n","    feature, _ = self.lstm(embedded)\n","    oupt = self.linear(feature)\n","    return oupt"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfQvJ0cJOLWx","executionInfo":{"status":"ok","timestamp":1616558101477,"user_tz":-480,"elapsed":786,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def transform(text, targets):\n","  li = ['company', 'date', 'address', 'total']\n","  res = torch.zeros(len(text), 1)\n","  for ind, value in enumerate(li):\n","    index = text.upper().find(targets[value][0:5])\n","    end = index+len(targets[value])\n","    res[index:end, 0] = torch.LongTensor([ind+1 for _ in range(len(targets[value]))])\n","\n","  text = etfo\n","  temp_tensor = torch.zeros(len(text), 1, dtype=torch.long)\n","  temp_tensor[:, 0] = torch.LongTensor([VOCAB.find(c) for c in text])\n","\n","  mask = temp_tensor[:,0] >= 0\n","  indices = torch.nonzero(mask)\n","  res = res[indices[:,0]]\n","\n","  return res.squeeze().type(torch.LongTensor)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUlOJxjqN7Hm","executionInfo":{"status":"ok","timestamp":1616558101876,"user_tz":-480,"elapsed":473,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def train(epoch):\n","  \n","  epoch_loss = 0.0\n","  num_epochs = 15\n","  best_loss = 999999\n","  best_epoch = -1\n","  \n","  temp = []\n","  ind1, ind2 = 0, 0\n","  for epoch in range(num_epochs):\n","\n","    running_loss = 0.0\n","    files = glob.glob(f\"/content/drive/MyDrive/Job/Infrred/dataset/train_all/*\")\n","    for file in files: \n","      # forward\n","      if file.endswith('jpg'):\n","        img = img_preprocess(file, 1)\n","        text = img_words(img)\n","        text = text.upper()\n","        text_tensor = get_text_tensor(text)\n","        output = model(text_tensor)\n","        prob = torch.nn.functional.softmax(output, dim=2)\n","        prob = prob.squeeze()\n","        ind1 = 1\n","\n","      # label\n","      if file.endswith('json'):\n","        temp = pd.read_json(file, typ='series')\n","        target = transform(text, temp)\n","        ind2 = 1\n","\n","      if ind1 == 1 and ind2 == 1:\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # backward + optimize\n","        loss = criterion(prob, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","      # # print statistics\n","      # running_loss += loss.item()\n","      # if i % 2000 == 1999:    # print every 2000 mini-batches\n","      #       print('[%d, %5d] loss: %.3f' %\n","      #             (epoch + 1, i + 1, running_loss / 2000))\n","      #       running_loss = 0.0\n","\n","  print('Finished Training')"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"y17DLaaB9yt9","executionInfo":{"status":"ok","timestamp":1616558654252,"user_tz":-480,"elapsed":663,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["# convert text to text tensor\n","def get_text_tensor(text):\n","  \n","  text = text.upper()\n","  text_tensor = torch.zeros(len(text), 1, dtype=torch.long)\n","  text_tensor[:, 0] = torch.LongTensor([all_vocab.find(c) for c in text])\n","\n","  # make sure that the nums in the tensor are in the range\n","  mask = text_tensor[:,0] >= 0\n","  indices = torch.nonzero(mask)\n","  text_tensor = text_tensor[indices[:,0]]\n","\n","  return text_tensor"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWbpTIWsH15c","executionInfo":{"status":"ok","timestamp":1616558658858,"user_tz":-480,"elapsed":679,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["# convert prediction to dictionary of each field thru the cosecutive number in pred and the max_prob in the corresponding sequence\n","def pred_to_dict(text, pred, prob):\n","  res = {\"company\": (\"\", 0), \"date\": (\"\", 0), \"address\": (\"\", 0), \"total\": (\"\", 0)}\n","  keys = list(res.keys())\n","\n","  cons_pred = [0] + (np.nonzero(np.diff(pred))[0] + 1).tolist() + [len(pred)] \n","\n","  for i in range(len(cons_pred) - 1):\n","      pred_class = pred[cons_pred[i]] - 1\n","      # no class\n","      if pred_class == -1:\n","        continue\n","\n","      cur_key = keys[pred_class]\n","      cur_prob = prob[cons_pred[i]:cons_pred[i+1]].max()\n","      if cur_prob > res[cur_key][1]:\n","        res[cur_key] = (text[cons_pred[i] : cons_pred[i+1]], cur_prob)\n","    \n","  return {k: regex.sub(r\"[\\t\\n]\", \" \", v[0].strip()) for k, v in res.items()}"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8okTMQ0KFvn","executionInfo":{"status":"ok","timestamp":1616558661191,"user_tz":-480,"elapsed":659,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def predict(model, text, text_tensor):\n","\n","  with torch.no_grad():\n","    output = model(text_tensor)\n","    prob = torch.nn.functional.softmax(output, dim=2)\n","    prob, pred = torch.max(prob, dim=2)\n","    prob = prob.squeeze().numpy()\n","    pred = pred.squeeze().numpy()\n","    result = pred_to_dict(text, pred, prob)\n","  \n","  return result"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGphxbp4LzfD"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"Qetd89i7L2Sb","executionInfo":{"status":"ok","timestamp":1616570387943,"user_tz":-480,"elapsed":653,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def evaluate(df, index, result):\n","  temp = df[df['index'] == int(index)]\n","  print(temp)\n","  company_truth.append(temp['company']) \n","  company_pred.append(result['company'])\n","  date_truth.append(temp['date']) \n","  date_pred.append(result['date'])\n","  address_truth.append(temp['address']) \n","  address_pred.append(result['address'])\n","  total_truth.append(temp['total']) \n","  total_pred.append(result['total'])"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtjZojW_NK8r","executionInfo":{"status":"ok","timestamp":1616570560807,"user_tz":-480,"elapsed":654,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def compute_f1_and_precision(company_truth, company_pred, date_truth, date_pred, address_truth, address_pred, total_truth, total_pred):\n","\n","  company_score = f1_score(company_truth, company_pred, average='macro')\n","  date_score = f1_score(date_truth, date_pred, average='macro')\n","  address_score = f1_score(address_truth, address_pred, average='macro')\n","  total_score = f1_score(total_truth, total_pred, average='macro')\n","  f1 = [company_score, date_score, address_score, total_score]\n","\n","  company_pre = precision_score(company_truth, company_pred, average='macro')\n","  date_pre = precision_score(date_truth, date_pred, average='macro')\n","  address_pre = precision_score(address_truth, address_pred, average='macro')\n","  total_pre = precision_score(total_truth, total_pred, average='macro')\n","  precision = [company_pre, date_pre, address_pre, total_pre]\n","\n","  return f1, precision"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5_XyqOUMtC1","executionInfo":{"status":"ok","timestamp":1616570773686,"user_tz":-480,"elapsed":654,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["def plot_result(result, title):\n","  fig = plt.figure()\n","  ax = fig.add_axes([1,0,1,1])\n","  langs = ['Company', 'Date', 'Address', 'Total']\n","  ax.bar(langs,result)\n","  plt.title(title)\n","  plt.show()"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEZu0wL0Md0J"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"8PDvX12MyY1T","executionInfo":{"status":"ok","timestamp":1616583417399,"user_tz":-480,"elapsed":2243175,"user":{"displayName":"Chun-Liang Wu","photoUrl":"","userId":"12330002015168332955"}}},"source":["if __name__ == \"__main__\":\n","\n","  all_vocab = ascii_uppercase+digits+punctuation+\" \\t\\n\"\n","  model = MyModel(len(all_vocab), 16, 256)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","  company_truth, company_pred, date_truth, date_pred, address_truth, address_pred, total_truth, total_pred = [], [], [], [], [], [], [], []\n","\n","  model.load_state_dict(torch.load(\"/content/drive/MyDrive/Job/Infrred/AIESI-master/AIESI_using_tesseract/model.pth\"))\n","  # all json files\n","  df = pd.read_csv('/content/drive/MyDrive/Job/Infrred/dataset/train_all/all_json.csv', index_col=0)\n","  # train model\n","  # train(5)\n","  # torch.save(model.state_dict(), '/content/drive/MyDrive/Job/Infrred/dataset')\n","\n","  # # compute score for training dataset\n","  # files = glob.glob('/content/drive/MyDrive/Job/Infrred/dataset/train_all/*')\n","  # for file in files:\n","  #   if file.endswith('jpg'):\n","  #     # return index\n","  #     index = file[-7:-4]\n","  #     if index.find('/') != -1:\n","  #       index = index[index.find('/')+1:]\n","\n","  #     img = img_preprocess(file, 1)\n","  #     text = img_words(img)\n","  #     text = text.upper()\n","  #     text_tensor = get_text_tensor(text)\n","  #     pred = predict(model, text, text_tensor)\n","  #     print(pred)\n","  #     evaluate(df, index, pred)\n","\n","  #     print(index)\n","  # # print(company_truth, company_pred, date_truth, date_pred, address_truth, address_pred, total_truth, total_pred)\n","  # f1, precision = compute_f1_and_precision(company_truth, company_pred, date_truth, date_pred, address_truth, address_pred, total_truth, total_pred)\n","  # plot_result(f1, 'F1 Score')\n","  # plot_result(precision, 'Precision')\n","\n","\n","  # output json for validation data set \n","  files = glob.glob('/content/drive/MyDrive/Job/Infrred/dataset/val_all/*')\n","  df = pd.DataFrame(columns=['index', 'company', 'date', 'address', 'total'])\n","  for file in files:\n","    if file.endswith('jpg'):\n","        # return index\n","      index = file[-7:-4]\n","      if index.find('/') != -1:\n","        index = index[index.find('/')+1:]\n","\n","      img = img_preprocess(file, 1)\n","      text = img_words(img)\n","      text_tensor = get_text_tensor(text)\n","      pred = predict(model, text, text_tensor)\n","      pred['index'] = index\n","      pred = pd.DataFrame([pred])\n","      df = pd.concat([df, pred])\n","\n","  df.to_csv('/content/drive/MyDrive/Job/Infrred/dataset/val_all/val_all_json.csv')"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"hl0VXxK9u1QJ"},"source":[""],"execution_count":null,"outputs":[]}]}